# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CUiDWD7bkA5h-ATbmrItsfTlXIfVFpaR
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install -U accelerate
! pip install -U transformers

from transformers import BertForMaskedLM, LineByLineTextDataset, BertTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments

import accelerate
print(accelerate.__version__)

model = BertForMaskedLM.from_pretrained('bert-base-uncased')
tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')
dataset = LineByLineTextDataset(tokenizer=tokeniser, file_path='/content/drive/MyDrive/THESIS/CODE/finetuning_dataset_pt1.txt', block_size=128)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokeniser, mlm=True, mlm_probability=0.15)
training_args = TrainingArguments(output_dir = '/content/drive/MyDrive/THESIS/CODE/MODELS', overwrite_output_dir=True, num_train_epochs=1, per_device_train_batch_size=64, save_steps=10000, save_total_limit=2, prediction_loss_only=True)
trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset)

trainer.train()
trainer.save_model('/content/drive/MyDrive/THESIS/CODE/MODELS')

import torch
import string

top_k = 20

top_k = 1

bert_tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/THESIS/CODE/MODELS', output_attentions = True).eval()


def decode(tokeniser, pred_idx, top_clean):
    ignore_tokens = string.punctuation + '[PAD]'
    tokens = []
    for w in pred_idx:
        token = ''.join(tokeniser.decode(w).split())
        if token not in ignore_tokens:
            tokens.append(token.replace('##', ''))
    return '\n'.join(tokens[:top_clean])


def encode(tokeniser, text_sentence, add_special_tokens=True):
    #print(f'text sentence before replacing the token: {text_sentence}')
    text_sentence = text_sentence.replace('<mask>', tokeniser.mask_token)
    #print(f'text sentence after replacing the token: {text_sentence}')
    if tokeniser.mask_token == text_sentence.split()[-1]:
        text_sentence += ' .'

    input_ids = torch.tensor([tokeniser.encode(text_sentence, add_special_tokens=add_special_tokens)])

    print("Input IDs:", input_ids)

    mask_idx = torch.where(input_ids == tokeniser.mask_token_id)[1].tolist()

    print("Mask index:", mask_idx)

    if not mask_idx:
        print(f'mask idx not found')

    return input_ids, mask_idx[0]


def get_all_predictions(text_sentence, top_clean=5):
    input_ids, mask_idx = encode(bert_tokeniser, text_sentence)
    with torch.no_grad():
        predict = bert_model(input_ids)

    attentions = predict[-1]
    tokens = bert_tokeniser.convert_ids_to_tokens(input_ids[0])
    bert = decode(bert_tokeniser, predict[0][0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)
    return {'bert': bert, 'attentions': attentions, 'tokens': tokens}


def get_prediction(input_text):
    res = get_all_predictions(input_text, top_clean = int(top_k))
    return res

